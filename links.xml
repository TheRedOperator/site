<?xml version="1.0" encoding="UTF-8"?>
<links>
  <link>
    <id>1</id>
    <url>https://university.apisec.ai</url>
    <title>APIsec University – Become an API Security Expert</title>
    <description>APIsec University provides free training courses on API Security. Learn how to find API vulnerabilities and keep them secure.</description>
    <tag>Redteam</tag>
  </link>
  <link>
    <id>2</id>
    <url>https://llmtop10.com</url>
    <title>OWASP | Top 10 for Large Language Model Applications</title>
    <tag>Pentest</tag>
  </link>
  <link>
    <id>3</id>
    <url>https://embracethered.com/blog/posts/2020/machine-learning-attack-series-overview/</url>
    <title>Machine Learning Attack Series: Overview · Embrace The Red</title>
    <tag>Pentest</tag>
  </link>
  <link>
    <id>4</id>
    <url>http://d2l.ai/index.html</url>
    <title>Dive into Deep Learning — Dive into Deep Learning 1.0.3 documentation</title>
    <tag>Pentest</tag>
  </link>
  <link>
    <id>5</id>
    <url>https://nn.labml.ai/index.html</url>
    <title>Annotated Research Paper Implementations: Transformers, StyleGAN, Stable Diffusion, DDPM/DDIM, LayerNorm, Nucleus Sampling and more</title>
    <tag>Redteam</tag>
  </link>
  <link>
    <id>6</id>
    <url>https://lightning.ai</url>
    <title>Lightning AI</title>
    <description>The all-in-one platform for AI development. Code together. Prototype. Train. Scale. Serve. From your browser - with zero setup. From the creators of PyTorch Lightning.</description>
    <tag>Redteam</tag>
  </link>
  <link>
    <id>7</id>
    <url>http://introtodeeplearning.com</url>
    <title>MIT Deep Learning 6.S191</title>
    <tag>Pentest</tag>
  </link>
  <link>
    <id>8</id>
    <url>https://learnprompting.org/</url>
    <title>Learn Prompting | Generative AI, Prompt Engineering, &amp; Free Online Courses</title>
    <description>Learn Generative AI, Prompt Engineering, and ChatGPT skills to future-proof your career with our free &amp; paid courses, quizzes, and projects. Free 3 Day Plus Trial. Beginner-Friendly.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>9</id>
    <url>https://www.promptingguide.ai/</url>
    <title>Prompt Engineering Guide | Prompt Engineering Guide&lt;!-- --&gt;</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>10</id>
    <url>https://www.lesswrong.com/</url>
    <title>LessWrong</title>
    <description>A community blog devoted to refining the art of rationality</description>
    <tag>Redteam</tag>
  </link>
  <link>
    <id>11</id>
    <url>https://github.com/jiep/offensive-ai-compilation</url>
    <title>GitHub - jiep/offensive-ai-compilation: A curated list of useful resources that cover Offensive AI.</title>
    <description>A curated list of useful resources that cover Offensive AI. - jiep/offensive-ai-compilation</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>12</id>
    <url>https://lmql.ai/</url>
    <title>LMQL is a programming language for LLM interaction. | LMQL</title>
    <description>Language Model Query Language</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>13</id>
    <url>https://huggingface.co/course/chapter0/1</url>
    <title>Introduction - Hugging Face NLP Course</title>
    <description>We’re on a journey to advance and democratize artificial intelligence through open source and open science.</description>
    <tag>Tips &amp; Tricks</tag>
  </link>
  <link>
    <id>14</id>
    <url>https://course.fast.ai/</url>
    <title>Practical Deep Learning for Coders - Practical Deep Learning</title>
    <description>A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>15</id>
    <url>https://airisk.io</url>
    <title>AI Risk Database</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>16</id>
    <url>https://blog.eleuther.ai/transformer-math/</url>
    <title>Transformer Math 101 | EleutherAI Blog</title>
    <description>We present basic math related to computation and memory usage for transformers</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>17</id>
    <url>https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning</url>
    <title>Failure Modes in Machine Learning | Microsoft Learn</title>
    <description>Machine Learning Threat Taxonomy</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>18</id>
    <url>https://microsoft.github.io/AI-For-Beginners/</url>
    <title>AI for Beginners</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>19</id>
    <url>https://atlas.mitre.org</url>
    <title>MITRE | ATLAS™</title>
    <tag>Tips &amp; Tricks</tag>
  </link>
  <link>
    <id>20</id>
    <url>https://airisk.io</url>
    <title>AI Risk Database</title>
    <tag>Redteam</tag>
  </link>
  <link>
    <id>21</id>
    <url>https://github.com/udlbook/udlbook</url>
    <title>GitHub - udlbook/udlbook: Understanding Deep Learning - Simon J.D. Prince</title>
    <description>Understanding Deep Learning - Simon J.D. Prince. Contribute to udlbook/udlbook development by creating an account on GitHub.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>22</id>
    <url>https://github.com/tigthor/neural-network-hacking</url>
    <title>GitHub - tigthor/neural-network-hacking: Hacking the Singularity. Deep learning hacking. Weaponizing AI in Offensive security</title>
    <description>Hacking the Singularity. Deep learning hacking. Weaponizing AI in Offensive security - tigthor/neural-network-hacking</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>23</id>
    <url>https://nicholas.carlini.com/</url>
    <title>Nicholas Carlini</title>
    <description>Nicholas Carlini is a research scientist at Google DeepMind working at the intersection of machine learning and computer security.</description>
    <tag>Pentest</tag>
  </link>
  <link>
    <id>24</id>
    <url>https://github.com/cleverhans-lab/cleverhans</url>
    <title>GitHub - cleverhans-lab/cleverhans: An adversarial example library for constructing attacks, building defenses, and benchmarking both</title>
    <description>An adversarial example library for constructing attacks, building defenses, and benchmarking both - cleverhans-lab/cleverhans</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>25</id>
    <url>https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/</url>
    <title>LLM Bootcamp - Spring 2023 - The Full Stack</title>
    <description>Learn best practices and tools for building LLM-powered apps</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>26</id>
    <url>https://kai-greshake.de/posts/in-escalating-order-of-stupidity/</url>
    <title>In Escalating Order of Stupidity</title>
    <description>In our recent paper on prompt injections, we derived new threats facing applications built on top of LLMs. In this post, I will take these abstract threat models and show how they will affect software being deployed to hundreds of millions of users- including nation-states and militaries. We will look at LLM applications in escalating order of stupidity, ending with attackers potentially compromising military LLMs ...</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>27</id>
    <url>https://github.com/brexhq/prompt-engineering</url>
    <title>GitHub - brexhq/prompt-engineering: Tips and tricks for working with Large Language Models like OpenAI&#39;s GPT-4.</title>
    <description>Tips and tricks for working with Large Language Models like OpenAI&#39;s GPT-4. - brexhq/prompt-engineering</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>28</id>
    <url>https://docs.cohere.com/docs/structure-of-the-course</url>
    <title>Structure of the Course</title>
    <description>NLP University has three main modules, and two appendices. Let me tell you about these one by one. But before, a bit about the structure of the course.Ways to learnThe material we’ve put together can be used in two ways: Sequential: If you want to start from the very beginning, the course will take ...</description>
    <tag>Pentest</tag>
  </link>
  <link>
    <id>29</id>
    <url>https://learn.microsoft.com/en-us/training/paths/pytorch-fundamentals/</url>
    <title>PyTorch Fundamentals - Training | Microsoft Learn</title>
    <description>Learn the fundamentals of deep learning with PyTorch! This beginner friendly learning path will introduce key concepts to building machine learning models in multiple domains include speech, vision, and natural language processing.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>30</id>
    <url>https://embracethered.com/blog/</url>
    <title>Embrace The Red · Embrace The Red</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>31</id>
    <url>https://mlsecops.com/podcast/mlsecops-red-teaming-threat-modeling-and-attack-methods-of-ai-apps</url>
    <title>MLSecOps: Red Teaming, Threat Modeling, and Attack Methods of AI Apps</title>
    <description>The MLSecOps Podcast speaks with Johann Rehberger, a distinguished Red Team Director, author, and blogger.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>32</id>
    <url>https://aisafety.info</url>
    <title>Stampy</title>
    <description>AI Safety FAQ</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>33</id>
    <url>https://aivillage.org/large%20language%20models/threat-modeling-llm/</url>
    <title>Threat Modeling LLM Applications - AI Village</title>
    <description>Threat Modeling LLM Applications Before we get started: Hi! My name is GTKlondike, and these are my opinions as a cybersecurity consultant. While experts from the AI Village provided input, I will always welcome open discussion so that we can come to a better understanding of LLM security together. If you’d like to continue this conversation, you can reach me on Twitter at @GTKlondike. And ...</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>34</id>
    <url>https://www.splunk.com/en_us/blog/security/paws-in-the-pickle-jar-risk-vulnerability-in-the-model-sharing-ecosystem.html</url>
    <title>Paws in the Pickle Jar: Risk &amp; Vulnerability in the Model-sharing Ecosystem | Splunk</title>
    <description>As AI / Machine Learning (ML) systems now support millions of daily users, has our understanding of the relevant security risks kept pace with this wild rate of adoption?</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>35</id>
    <url>https://devsecopsguides.github.io/docs/rules/llm/</url>
    <title>LLM | DevSecOps Guides</title>
    <description>Guides for DevSecOps</description>
    <tag>Redteam</tag>
  </link>
  <link>
    <id>36</id>
    <url>https://github.com/FonduAI/awesome-prompt-injection</url>
    <title>GitHub - FonduAI/awesome-prompt-injection: Learn about a type of vulnerability that specifically targets machine learning models</title>
    <description>Learn about a type of vulnerability that specifically targets machine learning models - FonduAI/awesome-prompt-injection</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>37</id>
    <url>https://positive.security/blog/auto-gpt-rce</url>
    <title>Hacking Auto-GPT and escaping its docker container | Positive Security</title>
    <description>We leverage indirect prompt injection to trick Auto-GPT (GPT-4) into executing arbitrary code when it is asked to perform a seemingly harmless task such as text summarization on a malicious website, and discovered vulnerabilities that allow escaping its sandboxed execution environment.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>38</id>
    <url>https://github.com/OWASP/www-project-top-10-for-large-language-model-applications</url>
    <title>GitHub - OWASP/www-project-top-10-for-large-language-model-applications: OWASP Foundation Web Respository</title>
    <description>OWASP Foundation Web Respository. Contribute to OWASP/www-project-top-10-for-large-language-model-applications development by creating an account on GitHub.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>39</id>
    <url>https://hackstery.com/</url>
    <title>hackstery - exploring AI security</title>
    <description>Discover insights on AI &amp; LLM security. Stay informed about the latest vulnerabilities and attacks. Explore the world of AI security with our newsletter &amp; blog.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>40</id>
    <url>https://adversarial-robustness-toolbox.readthedocs.io/en/latest/</url>
    <title>Welcome to the Adversarial Robustness Toolbox — Adversarial Robustness Toolbox 1.17.0 documentation</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>41</id>
    <url>https://llm-attacks.org/</url>
    <title>Universal and Transferable Attacks on Aligned Language Models</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>42</id>
    <url>https://chainforge.ai/docs/</url>
    <title>ChainForge Documentation</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>43</id>
    <url>https://github.com/bkitano/llama-from-scratch</url>
    <title>GitHub - bkitano/llama-from-scratch: Llama from scratch, or How to implement a paper without crying</title>
    <description>Llama from scratch, or How to implement a paper without crying - bkitano/llama-from-scratch</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>44</id>
    <url>https://www.arxiv-vanity.com</url>
    <title>arXiv Vanity</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>45</id>
    <url>https://lilianweng.github.io/posts/2023-06-23-agent/</url>
    <title>LLM Powered Autonomous Agents | Lil&#39;Log</title>
    <description>Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s ...</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>46</id>
    <url>https://5stars217.github.io/2023-08-08-red-teaming-with-ml-models/</url>
    <title>Model Confusion - Weaponizing ML models for red teams and bounty hunters</title>
    <description>How I hacked a bunch of companies via machine learning attacks.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>47</id>
    <url>https://github.com/JosephTLucas/HackThisAI</url>
    <title>GitHub - JosephTLucas/HackThisAI: Adversarial Machine Learning (AML) Capture the Flag (CTF)</title>
    <description>Adversarial Machine Learning (AML) Capture the Flag (CTF) - JosephTLucas/HackThisAI</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>48</id>
    <url>https://hadess.io/owasp-top-10-for-large-language-model-applications/</url>
    <title>OWASP In Cyber Security 2023 | Top 10 For LLM Applications – HADESS</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>49</id>
    <url>https://lena-voita.github.io/nlp_course.html</url>
    <title>NLP Course | For You</title>
    <description>Natural Language Processing course with interactive lectures-blogs, research thinking exercises and related papers with summaries. Also a lot of fun inside!</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>50</id>
    <url>https://nicholas.carlini.com/</url>
    <title>Nicholas Carlini</title>
    <description>Nicholas Carlini is a research scientist at Google DeepMind working at the intersection of machine learning and computer security.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>51</id>
    <url>https://llmsecurity.net/</url>
    <title>LLM Security</title>
    <description>The latest research, news, and papers on large language model security.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>52</id>
    <url>https://www.docdroid.net/KfwKd1y/llm-redteaming-pdf</url>
    <title>llm redteaming.pdf | DocDroid</title>
    <description>Structured LLM Red-teaming. Leon Derczynski March 2023. Structured LLM Red-teaming What is language model red teaming? Scoping LLM assessment Designing the exercise To Battle! — what’s in the “war chest”? Post-exercise. Intro: How risky is our model?. ...</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>53</id>
    <url>https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e</url>
    <title>Normcore LLM Reads · GitHub</title>
    <description>Normcore LLM Reads. GitHub Gist: instantly share code, notes, and snippets.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>54</id>
    <url>https://wiki.hego.tech/owasp/owasp-llm-top-10-v1.0</url>
    <title>OWASP LLM Top 10 V1.0 - HEGO Wiki</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>55</id>
    <url>https://lightning.ai/pages/llm-learning-lab/</url>
    <title>LLM Learning Lab - Lightning AI</title>
    <description>Immerse yourself in a curated collection of blogs, tutorials, and how-to videos to help you unlock the transformative potential of large language models.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>56</id>
    <url>https://github.com/jedi4ever/learning-llms-and-genai-for-dev-sec-ops</url>
    <title>GitHub - jedi4ever/learning-llms-and-genai-for-dev-sec-ops: A set of lessons aimed at anyone learning LLM and generative AI concepts, with sections on operations and security, as well as development.</title>
    <description>A set of lessons aimed at anyone learning LLM and generative AI concepts, with sections on operations and security, as well as development. - jedi4ever/learning-llms-and-genai-for-dev-sec-ops</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>57</id>
    <url>https://pytorch.org/blog/inside-the-matrix/</url>
    <title>Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond | PyTorch</title>
    <description>Use 3D to visualize matrix multiplication expressions, attention heads with real weights, and more.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>58</id>
    <url>https://udlbook.github.io/udlbook/</url>
    <title>udlbook</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>59</id>
    <url>https://www.mbmlbook.com/</url>
    <title>Model-Based Machine Learning: an online book</title>
    <description>Machine learning book which uses a model-based approach.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>60</id>
    <url>https://github.com/stas00/ml-engineering</url>
    <title>GitHub - stas00/ml-engineering: Machine Learning Engineering Open Book</title>
    <description>Machine Learning Engineering Open Book. Contribute to stas00/ml-engineering development by creating an account on GitHub.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>61</id>
    <url>https://wiki.offsecml.com/Welcome+to+the+Offensive+ML+Framework</url>
    <title>Welcome to the Offensive ML Playbook - OffSecML Playbook</title>
    <description>Welcome to the Offensive ML Playbook - OffSecML Playbook</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>62</id>
    <url>https://github.com/google/model-transparency#model-transparency</url>
    <title>GitHub - google/model-transparency: Supply chain security for ML</title>
    <description>Supply chain security for ML. Contribute to google/model-transparency development by creating an account on GitHub.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>63</id>
    <url>https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31/code</url>
    <title>AI Village Capture the Flag @ DEFCON31 | Kaggle</title>
    <description>Collect flags by evading, poisoning, stealing, and fooling AI/ML</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>64</id>
    <url>https://github.com/protectai/ai-exploits</url>
    <title>GitHub - protectai/ai-exploits: A collection of real world AI/ML exploits for responsibly disclosed vulnerabilities</title>
    <description>A collection of real world AI/ML exploits for responsibly disclosed vulnerabilities - GitHub - protectai/ai-exploits: A collection of real world AI/ML exploits for responsibly disclosed vulnerabilities</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>65</id>
    <url>https://wiki.offsecml.com/Welcome+to+the+Offensive+ML+Playbook</url>
    <title>Welcome to the Offensive ML Playbook - OffSecML Playbook</title>
    <description>Welcome to the Offensive ML Playbook - OffSecML Playbook</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>66</id>
    <url>http://paper.hackaprompt.com/</url>
    <title>HackAPrompt</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>67</id>
    <url>https://spinningup.openai.com/en/latest/</url>
    <title>Welcome to Spinning Up in Deep RL! — Spinning Up documentation</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>68</id>
    <url>https://github.com/stas00/ml-engineering</url>
    <title>GitHub - stas00/ml-engineering: Machine Learning Engineering Open Book</title>
    <description>Machine Learning Engineering Open Book. Contribute to stas00/ml-engineering development by creating an account on GitHub.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>69</id>
    <url>https://lve-project.org/index.html</url>
    <title>LVE Repository</title>
    <description>We document and track vulnerabilities and exposures of large language models (LVEs).</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>70</id>
    <url>https://arxiv.org/abs/2312.08365</url>
    <title>[2312.08365] An Invitation to Deep Reinforcement Learning</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>71</id>
    <url>https://www.baseten.co/blog/llm-transformer-inference-guide/</url>
    <title>A guide to LLM inference and performance</title>
    <description>Learn if LLM inference is compute or memory bound to fully utilize GPU power. Get insights on better GPU resource utilization.</description>
    <tag>Misc</tag>
  </link>
  <link>
    <id>72</id>
    <url>https://www.robustintelligence.com/blog-posts/using-ai-to-automatically-jailbreak-gpt-4-and-other-llms-in-under-a-minute</url>
    <title>Using AI to Automatically Jailbreak GPT-4 and Other LLMs in Under a Minute — Robust Intelligence</title>
    <tag>Misc</tag>
  </link>
  <link>
    <id>73</id>
    <url>https://chats-lab.github.io/persuasive_jailbreaker/#</url>
    <title>How Johnny Can Persuade LLMs to Jailbreak Them:&lt;br&gt;Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs</title>
    <description>We study how to persuade LLMs to jailbreak them and advocate for more fundamental mitigation for highly interactive LLMs</description>
    <tag> K8s</tag>
  </link>
  <link>
    <id>74</id>
    <url>https://llm-tuning-safety.github.io/</url>
    <title>LLM Finetuning Risks</title>
    <tag> K8s</tag>
  </link>
  <link>
    <id>75</id>
    <url>https://confaide.github.io/</url>
    <title>Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory</title>
    <description>Contextual Privacy in LLMs</description>
    <tag>Pentest</tag>
  </link>
  <link>
    <id>76</id>
    <url>https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/</url>
    <title>LeftoverLocals: Listening to LLM responses through leaked GPU local memory | Trail of Bits Blog</title>
    <description>By Tyler Sorensen and Heidy Khlaaf We are disclosing LeftoverLocals: a vulnerability that allows recovery of data from GPU local memory created by another process on Apple, Qualcomm, AMD, and Imagination GPUs. LeftoverLocals impacts the security posture of GPU applications as a whole, with particular significance to LLMs and ML models run on impacted GPU…</description>
    <tag>Redteam</tag>
  </link>
  <link>
    <id>77</id>
    <url>http://llm-chronicles.com/prompt-injection-talk/</url>
    <title>Prompt Injection</title>
    <tag>Tips &amp; Tricks</tag>
  </link>
  <link>
    <id>78</id>
    <url>https://github.com/stas00/ml-engineering</url>
    <title>GitHub - stas00/ml-engineering: Machine Learning Engineering Open Book</title>
    <description>Machine Learning Engineering Open Book. Contribute to stas00/ml-engineering development by creating an account on GitHub.</description>
    <tag>Tips &amp; Tricks</tag>
    <tag> K8s</tag>
    <tag>Pentest</tag>
  </link>
</links>